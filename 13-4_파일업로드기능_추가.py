# 0. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ

import streamlit as st
import tempfile

from langchain.document_loaders import PyPDFLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import ChatOllama
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


# 1. ì—…ë¡œë“œëœ PDF ë¬¸ì„œ ì²˜ë¦¬
@st.cache_resource
def load_pdf(_file):
    # ì„ì‹œ íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ì—…ë¡œë“œëœ PDF íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ì €ì¥
    # tempfile : ì„ì‹œ íŒŒì¼ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©í•˜ëŠ” python í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
    # NamedTemporaryFile : ì´ë¦„ì´ ìˆëŠ” ì„ì‹œ íŒŒì¼ì„ ìƒì„±
    # mode="wb" : íŒŒì¼ì„ ì“°ê¸° ëª¨ë“œ(w)ë¡œ ì—´ë˜, ë°”ì´ë„ˆë¦¬ ëª¨ë“œ(b)ë¡œ ì—´ê¸° -> ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, PDF ë“±ê³¼ ê°™ì€ ë°”ì´ë„ˆë¦¬ íŒŒì¼ì„ ë‹¤ë£° ë•Œ ì‚¬ìš©
    # delete=False : ì„ì‹œ íŒŒì¼ì„ ë‹«ì•„ë„ ì‚­ì œí•˜ì§€ ì•Šë„ë¡ ì„¤ì •
    # delete=Trueë¡œ ì„¤ì •í•˜ë©´, íŒŒì¼ì„ ë‹«ì„ ë•Œ ìë™ìœ¼ë¡œ ì‚­ì œë¨
    with tempfile.NamedTemporaryFile(mode="wb", delete=False) as tmp_file:
        # ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë‚´ìš©ì„ ì„ì‹œ íŒŒì¼ì— ê¸°ë¡
        tmp_file.write(_file.getvalue())
        # ì„ì‹œ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ë³€ìˆ˜ì— ì €ì¥
        tmp_file_path = tmp_file.name
        # ì„ì‹œ íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ë¡œë“œ
        loader = PyPDFLoader(file_path=tmp_file_path)
    return loader.load()

@st.cache_resource
def create_vector_store(_docs):
    embeddings = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    text_splitter = SemanticChunker(embeddings)
    split_docs = text_splitter.split_documents(_docs)
    vectorstore = Chroma.from_documents(split_docs, embeddings)
    return vectorstore

def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)


# 2. RAG ì²´ì¸ êµ¬ì„±
@st.cache_resource
def chaining(_pages):
    vectorstore = create_vector_store(_pages)

    llm = ChatOllama(
        model = "basic_kanana",
        temperature = 0.7
    )
    
    retriever_from_llm = MultiQueryRetriever.from_llm(
        retriever = vectorstore.as_retriever(
            search_type = "mmr",
            search_kwargs = {"lambda_mult": 0.7, "fetch_k": 10, "k": 3}
        ),
        llm = llm
    )

    qa_system_prompt = """
    ë‹¹ì‹ ì€ ì§ˆë¬¸ ì‘ë‹µ ì‘ì—…ì„ ìœ„í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \
    ì•„ë˜ì— ì œê³µëœ ê²€ìƒ‰ëœ ë¬¸ë§¥ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. \
    ë‹µì„ ëª¨ë¥¼ ê²½ìš°, ëª¨ë¥¸ë‹¤ê³  ì†”ì§í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”. \
    ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.\
    {context}
    """

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("human", "{input}"),
        ]
    )

    rag_chain = (
        {"context": retriever_from_llm | format_docs, "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain


# 3. Streamlit UI êµ¬ì„±
st.header("ChatPDF ğŸ’¬ ğŸ“š")
uploaded_file = st.file_uploader("Upload a PDF", type=["pdf"])  # st.file_uploader("íŒŒì¼ ì—…ë¡œë”ì˜ ì œëª©", type=["ì›í•˜ëŠ” íƒ€ì…"])
if uploaded_file is not None:
    pages = load_pdf(uploaded_file)

    rag_chain = chaining(pages)

    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

    for msg in st.session_state.messages:
        st.chat_message(msg['role']).write(msg['content'])

    if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
        st.chat_message("human").write(prompt_message)
        st.session_state.messages.append({"role": "user", "content": prompt_message})
        with st.chat_message("ai"):
            with st.spinner("Thinking..."):
                response = rag_chain.invoke(prompt_message)
                st.session_state.messages.append({"role": "assistant", "content": response})
                st.write(response)